{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the Energy Intensity of Running Tabular DDPMs ##\n",
    "\n",
    "Things we care about:\n",
    "1) Model Size\n",
    "2) Model Type (CNN or MLP)\n",
    "3) Dataset size\n",
    "4) Batch Size\n",
    "5) Data Type (Image or Tabular)\n",
    "6) Training time\n",
    "7) Sampling time\n",
    "8) Sampled Data size\n",
    "\n",
    "We hope to use the experiment run with this script to evaluate the carbon intensity of DDPMs\n",
    "\n",
    "Training: Run DDPM training until convergence\n",
    "\n",
    "Sampling: Generate 8192 samples with batch size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the goods\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ImgDiffusionTools as img_ddpm\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "\n",
    "-Run\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "clock = end - start\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:00<00:00, 8565.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 1, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:00<00:00, 3813.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "#Set up the loop:\n",
    "\n",
    "\n",
    "machine_id = 'Quadro_RTX_4000_Shannon'\n",
    "DS_sizes = [2048,4096]\n",
    "batch_size = 64\n",
    "\n",
    "timesteps = 100\n",
    "patience = 100\n",
    "\n",
    "#Net_sizes = [1,2,4,8]\n",
    "\n",
    "Net_sizes = [1,2]\n",
    "\n",
    "t_dim = 32\n",
    "\n",
    "\n",
    "# Load in image data:\n",
    "Wave_64 = []\n",
    "for i in tqdm(range(0,2048)):\n",
    "    img = Image.open('./dataset/Wave_4_Img_64/img_{}.png'.format(i))\n",
    "    a = np.array(img)\n",
    "    Wave_64.append([a])\n",
    "    img = Image.open('./dataset/Wave_8_Img_64/img_{}.png'.format(i))\n",
    "    a = np.array(img)\n",
    "    Wave_64.append([a])\n",
    "Wave_64 = (np.array(Wave_64).astype(np.float32) - 0)/255 * 2.0 - 1.0 # normalize to [-1,1]\n",
    "print(Wave_64.shape) # should be (8192,1,256,256)\n",
    "\n",
    "Wave_128= []\n",
    "for i in tqdm(range(0,2048)):\n",
    "    img = Image.open('./dataset/Wave_4_Img_128/img_{}.png'.format(i))\n",
    "    a = np.array(img)\n",
    "    Wave_128.append([a])\n",
    "    img = Image.open('./dataset/Wave_8_Img_128/img_{}.png'.format(i))\n",
    "    a = np.array(img)\n",
    "    Wave_128.append([a])\n",
    "Wave_128= (np.array(Wave_128).astype(np.float32) - 0)/255 * 2.0 - 1.0 # normalize to [-1,1]\n",
    "print(Wave_128.shape) # should be (4096,1,128,128)\n",
    "\n",
    "\n",
    "Wave = [Wave_64, Wave_128]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size:  4477\n",
      "Epoch: 0 Loss: 0.7094660997390747\n",
      "Epoch: 500 Loss: 0.15972065925598145\n",
      "Epoch: 1000 Loss: 0.15973961353302002\n",
      "Epoch: 1500 Loss: 0.16538196802139282\n",
      "Epoch: 2000 Loss: 0.16020259261131287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2262.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size:  13625\n",
      "Epoch: 0 Loss: 0.5631052851676941\n",
      "Epoch: 500 Loss: 0.07054269313812256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m epochs \u001b[39m=\u001b[39m I_model\u001b[39m.\u001b[39mtrain_loop_patience(patience, train_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ada/Documents/Carbon_Intensity_of_DGMs/Run_ImgDDPM_Experiment.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m clock_train \u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/Carbon_Intensity_of_DGMs/ImgDiffusionTools.py:340\u001b[0m, in \u001b[0;36mImg_DDPM_Env.train_loop_patience\u001b[0;34m(self, patience, train_loader)\u001b[0m\n\u001b[1;32m    337\u001b[0m batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    339\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps, (batch\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m],), device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 340\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_loss( batch, t)\n\u001b[1;32m    341\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Carbon_Intensity_of_DGMs/ImgDiffusionTools.py:363\u001b[0m, in \u001b[0;36mImg_DDPM_Env.get_loss\u001b[0;34m(self, x_0, t)\u001b[0m\n\u001b[1;32m    361\u001b[0m x_noisy, noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_diffusion_sample(x_0, t)\n\u001b[1;32m    362\u001b[0m x_noisy \u001b[39m=\u001b[39m x_noisy\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> 363\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x_noisy, t)\n\u001b[1;32m    364\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39ml1_loss(noise, noise_pred)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Carbon_Intensity_of_DGMs/ImgDiffusionTools.py:180\u001b[0m, in \u001b[0;36mSimpleUnet.forward\u001b[0;34m(self, x, timestep)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, timestep):\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m     \u001b[39m# Embedd time\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_mlp(timestep)\n\u001b[1;32m    181\u001b[0m     \u001b[39m# Initial conv\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv0(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(\u001b[39minput\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = ['run_id', 'model size', 'training epochs', 'train time', 'sample time 8192 samples']\n",
    "Results = []\n",
    "Results.append(labels)\n",
    "\n",
    "\n",
    "for i in range(len(Wave)):\n",
    "    for j in  range(len(DS_sizes)):\n",
    "     # Set up the training data:\n",
    "        DS = Wave[i][0:DS_sizes[j]]\n",
    "        img_size = int(DS.shape[2])\n",
    "        \n",
    "\n",
    "        train_loader = DataLoader(DS, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        for k in range(len(Net_sizes)):\n",
    "            torch.cuda.empty_cache()\n",
    "            I_model = img_ddpm.Img_DDPM_Env(img_size, Net_sizes[k], t_dim,timesteps, device)\n",
    "            #print(torch.cuda.memory_summary())\n",
    "            model_size = 0\n",
    "\n",
    "            for p in I_model.model.parameters():\n",
    "                model_size += p.numel()\n",
    "\n",
    "            print('Model size: ', model_size)\n",
    "\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            start = time.time()\n",
    "        \n",
    "            epochs = I_model.train_loop_patience(patience, train_loader)\n",
    "\n",
    "            end = time.time()\n",
    "            clock_train = end - start\n",
    "\n",
    "            # Sample the model\n",
    "            start = time.time()\n",
    "\n",
    "            X_gen = I_model.sample_images(64)\n",
    "\n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            clock_sample = end - start\n",
    "\n",
    "            id = 'Img_' + str(img_size) + '_DS_' + str(DS_sizes[j]) + '_Net_' + str(k) \n",
    "            np.save('./generated_samples/Shannon_RTX_4000/' + id + '_Samples_' + machine_id + '.npy', X_gen)\n",
    "\n",
    "            # Save a few images:\n",
    "            for l in range(0,32):\n",
    "                a = X_gen[l,0]\n",
    "                img = Image.fromarray(a)\n",
    "                path = './generated_samples/Shannon_RTX_4000/sample_images/'\n",
    "                img.save(path+ id + '_Sample_' + str(l) + '_' + machine_id + '.png')\n",
    "\n",
    "            Results.append([id,model_size, epochs, clock_train, clock_sample])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "path = './Img_Wave_Results_' + machine_id + '.csv'\n",
    "\n",
    "f =  open(path, 'w', newline='')\n",
    "writer = csv.writer(f)\n",
    "writer.writerows(Results)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
